{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4b0869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py2neo in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2021.2.3)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (2020.7.3)\n",
      "Requirement already satisfied: pygments>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (2.11.2)\n",
      "Requirement already satisfied: interchange~=2021.0.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (2021.0.4)\n",
      "Requirement already satisfied: monotonic in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (1.6)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (1.26.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (2022.9.14)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from py2neo) (21.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from interchange~=2021.0.4->py2neo) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging->py2neo) (3.0.9)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from wikipedia) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.9.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.1)\n",
      "Requirement already satisfied: spacy==3.0.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (1.21.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (1.7.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (63.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (8.0.17)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (2.28.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (1.0.9)\n",
      "Requirement already satisfied: pathy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (0.10.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy==3.0.3) (0.7.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy==3.0.3) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.0.3) (0.4.5)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.3) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->spacy==3.0.3) (2.0.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pathy->spacy==3.0.3) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install py2neo\n",
    "!pip install wikipedia\n",
    "!pip install spacy==3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de6a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
    "from py2neo.bulk import merge_nodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9265f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl (47.1 MB)\n",
      "     --------------------------------------- 47.1/47.1 MB 18.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from en-core-web-md==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.64.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.7.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.6)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pathy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (63.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.21.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.5)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (5.2.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c57014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_noun_chunks']\n"
     ]
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART','DATE','TIME','ORDINAL','LAW']\n",
    "#ENTITY_LABELS = ['CHARACTER','PLACE','HOUSE','TERMS','Family','Historical Event','RULE','SYMBOL','MOTTO','Title','Age','STATUS','ROLE']\n",
    "api_key = open('.api_key').read()\n",
    "\n",
    "non_nc = spacy.load('en_core_web_md')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "print(non_nc.pipe_names)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bec3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
    "    \n",
    "    text_ls = []\n",
    "    node_label_ls = []\n",
    "    url_ls = []\n",
    "    \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "        'key': api_key,\n",
    "    }   \n",
    "    \n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    if return_lists:\n",
    "        for element in response['itemListElement']:\n",
    "\n",
    "            try:\n",
    "                node_label_ls.append(element['result']['@type'])\n",
    "            except:\n",
    "                node_label_ls.append('')\n",
    "\n",
    "            try:\n",
    "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
    "            except:\n",
    "                text_ls.append('')\n",
    "                \n",
    "            try:\n",
    "                url_ls.append(element['result']['detailedDescription']['url'])\n",
    "            except:\n",
    "                url_ls.append('')\n",
    "                \n",
    "        return text_ls, node_label_ls, url_ls\n",
    "    \n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04812f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def create_svo_lists(doc, print_lists):\n",
    "    \n",
    "    subject_ls = []\n",
    "    verb_ls = []\n",
    "    object_ls = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            subject_ls.append((token.lower_, token.idx))\n",
    "        elif token.dep_ in VERBS:\n",
    "            verb_ls.append((token.lemma_, token.idx))\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            object_ls.append((token.lower_, token.idx))\n",
    "\n",
    "    if print_lists:\n",
    "        print('SUBJECTS: ', subject_ls)\n",
    "        print('VERBS: ', verb_ls)\n",
    "        print('OBJECTS: ', object_ls)\n",
    "    \n",
    "    return subject_ls, verb_ls, object_ls\n",
    "\n",
    "\n",
    "def remove_duplicates(tup, tup_posn):\n",
    "    \n",
    "    check_val = set()\n",
    "    result = []\n",
    "    \n",
    "    for i in tup:\n",
    "        if i[tup_posn] not in check_val:\n",
    "            result.append(i)\n",
    "            check_val.add(i[tup_posn])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_dates(tup_ls):\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    for entry in tup_ls:\n",
    "        if not entry[2].isdigit():\n",
    "            clean_tup_ls.append(entry)\n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_svo_triples(text, print_lists=False):\n",
    "    \n",
    "    clean_text = remove_special_characters(text)\n",
    "    doc = nlp(clean_text)\n",
    "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
    "    \n",
    "    graph_tup_ls = []\n",
    "    dedup_tup_ls = []\n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for subj in subject_ls: \n",
    "        for obj in object_ls:\n",
    "            \n",
    "            dist_ls = []\n",
    "            \n",
    "            for v in verb_ls:\n",
    "                \n",
    "                # Assemble a list of distances between each object and each verb\n",
    "                dist_ls.append(abs(obj[1] - v[1]))\n",
    "                \n",
    "            # Get the index of the verb with the smallest distance to the object \n",
    "            # and return that verb\n",
    "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
    "            \n",
    "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
    "            # later down in the process to allow for proper sentence recognition.\n",
    "\n",
    "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
    "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
    "            \n",
    "            # Add entries to the graph iff neither subject nor object is blank\n",
    "            if no_sw_subj and no_sw_obj:\n",
    "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
    "                graph_tup_ls.append(tup)\n",
    "        \n",
    "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
    "    \n",
    "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
    "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
    "    \n",
    "    return clean_tup_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a1a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_properties(tup_ls):\n",
    "    \n",
    "    init_obj_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "\n",
    "        try:\n",
    "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
    "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
    "        except:\n",
    "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
    "        \n",
    "        init_obj_tup_ls.append(new_tup)\n",
    "        \n",
    "    return init_obj_tup_ls\n",
    "\n",
    "\n",
    "def add_layer(tup_ls):\n",
    "\n",
    "    svo_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        \n",
    "        if tup[3]:\n",
    "            svo_tup = create_svo_triples(tup[3])\n",
    "            svo_tup_ls.extend(svo_tup)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return get_obj_properties(svo_tup_ls)\n",
    "        \n",
    "\n",
    "def subj_equals_obj(tup_ls):\n",
    "    \n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[0] != tup[2]:\n",
    "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
    "            \n",
    "    return new_tup_ls\n",
    "\n",
    "\n",
    "def check_for_string_labels(tup_ls):\n",
    "    # This is for an edge case where the object does not get fully populated\n",
    "    # resulting in the node labels being assigned to string instead of list.\n",
    "    # This may not be strictly necessary and the lines using it are commnted out\n",
    "    # below.  Run this function if you come across this case.\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        if isinstance(el[2], list):\n",
    "            clean_tup_ls.append(el)\n",
    "            \n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_word_vectors(tup_ls):\n",
    "\n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[3]:\n",
    "            doc = nlp(tup[3])\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
    "        else:\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
    "        new_tup_ls.append(new_tup)\n",
    "        \n",
    "    return new_tup_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f37a464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fire & Blood is a fantasy book by American writer George R. R. Martin and illustrated by Doug Wheatley. It tells the history of House Targaryen, the dynasty that ruled the Seven Kingdoms of Westeros in the backstory of his series A Song of Ice and Fire. Although originally planned for publication after the completion of the series, Martin has revealed his intent to publish the history in two volumes as the material had grown too large. The first volume was released on November 20, 2018.The second half of this first volume (an expanded version of The Princess and the Queen) has been adapted into the HBO series House of the Dragon, a prequel to Game of Thrones.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= wikipedia.summary('Fire and Blood')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb04a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a folder\n",
    "folder_name = \"wikidata\"\n",
    "os.mkdir(folder_name)\n",
    "\n",
    "# define the text content\n",
    "#text_content =wikipedia.summary('Fire and Blood')\n",
    "# save the text content in a txt file inside the folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e5bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(tup_ls):\n",
    "    \n",
    "    visited = set()\n",
    "    output_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if not tup[0] in visited:\n",
    "            visited.add(tup[0])\n",
    "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
    "            \n",
    "    return output_ls\n",
    "\n",
    "\n",
    "def convert_vec_to_ls(tup_ls):\n",
    "    \n",
    "    vec_to_ls_tup = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        vec_ls = [float(v) for v in el[4]]\n",
    "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
    "        vec_to_ls_tup.append(tup)\n",
    "        \n",
    "    return vec_to_ls_tup\n",
    "\n",
    "\n",
    "def add_nodes(tup_ls):   \n",
    "\n",
    "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
    "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
    "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14094649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges(edge_ls):\n",
    "    \n",
    "    edge_dc = {} \n",
    "    \n",
    "    # Group tuple by verb\n",
    "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
    "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
    "    \n",
    "    for tup in edge_ls: \n",
    "        if tup[1] in edge_dc: \n",
    "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
    "        else: \n",
    "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
    "    \n",
    "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
    "        \n",
    "        tx = graph.begin()\n",
    "        \n",
    "        for el in tup_ls:\n",
    "            source_node = nodes_matcher.match(name=el[0]).first()\n",
    "            target_node = nodes_matcher.match(name=el[2]).first()\n",
    "            if not source_node:\n",
    "                source_node = Node('Node', name=el[0])\n",
    "                tx.create(source_node)\n",
    "            if not target_node:\n",
    "                try:\n",
    "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
    "                    tx.create(target_node)\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                rel = Relationship(source_node, edge_labels, target_node)\n",
    "            except:\n",
    "                continue\n",
    "            tx.create(rel)\n",
    "        tx.commit()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff4dff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full\n",
    "def edge_tuple_creation(text):\n",
    "    \n",
    "    initial_tup_ls = create_svo_triples(text)\n",
    "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
    "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
    "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
    "    \n",
    "    return edges_word_vec_ls\n",
    "\n",
    "\n",
    "def node_tuple_creation(edges_word_vec_ls):\n",
    "    \n",
    "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
    "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
    "    #dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
    "    dedup_node_tup_ls = cleaned_node_tup_ls\n",
    "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
    "    \n",
    "    return node_tup_ls  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc20d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fireandblood_text = wikipedia.summary('Fire and Blood (Game of Thrones)')\n",
    "file_path = os.path.join(folder_name, \"tt1.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(fireandblood_text)\n",
    "fireandblood_edges_word_vec_ls = edge_tuple_creation(fireandblood_text)\n",
    "fireandblood_node_tup_ls = node_tuple_creation(fireandblood_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e507d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text2 = wikipedia.summary('Game of Thrones (season 1)')\n",
    "file_path = os.path.join(folder_name, \"tt2.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text2)\n",
    "text2_edges_word_vec_ls = edge_tuple_creation(text2)\n",
    "text2_node_tup_ls = node_tuple_creation(text2_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d432dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text3 = wikipedia.summary('A Song of Ice and Fire')\n",
    "file_path = os.path.join(folder_name, \"tt3.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text3)\n",
    "text3_edges_word_vec_ls = edge_tuple_creation(text3)\n",
    "text3_node_tup_ls = node_tuple_creation(text3_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a248eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text4 = wikipedia.summary(' List of A Song of Ice and Fire characters')\n",
    "file_path = os.path.join(folder_name, \"tt4.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text4)\n",
    "text4_edges_word_vec_ls = edge_tuple_creation(text4)\n",
    "text4_node_tup_ls = node_tuple_creation(text4_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6e3beb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text5 = wikipedia.summary(' Stone Age')\n",
    "file_path = os.path.join(folder_name, \"tt5.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text5)\n",
    "text5_edges_word_vec_ls = edge_tuple_creation(text5)\n",
    "text5_node_tup_ls = node_tuple_creation(text5_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2340e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text6 = wikipedia.summary('House of the Dragon')\n",
    "file_path = os.path.join(folder_name, \"tt6.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text6)\n",
    "text6_edges_word_vec_ls = edge_tuple_creation(text6)\n",
    "text6_node_tup_ls = node_tuple_creation(text6_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7846fe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text7 = wikipedia.summary('The Princess and the Queen')\n",
    "file_path = os.path.join(folder_name, \"tt7.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text7)\n",
    "text7_edges_word_vec_ls = edge_tuple_creation(text7)\n",
    "text7_node_tup_ls = node_tuple_creation(text7_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395ab2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text8 = wikipedia.summary('Robb Stark')\n",
    "file_path = os.path.join(folder_name, \"tt8.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text8)\n",
    "text8_edges_word_vec_ls = edge_tuple_creation(text8)\n",
    "text8_node_tup_ls = node_tuple_creation(text8_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46a97c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text9 = wikipedia.summary(\"House Stark\")\n",
    "file_path = os.path.join(folder_name, \"tt9.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text9)\n",
    "text9_edges_word_vec_ls = edge_tuple_creation(text9)\n",
    "text9_node_tup_ls = node_tuple_creation(text9_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d16b16e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text10 = wikipedia.summary(' Eddard Stark')\n",
    "file_path = os.path.join(folder_name, \"tt10.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text10)\n",
    "text10_edges_word_vec_ls = edge_tuple_creation(text10)\n",
    "text10_node_tup_ls = node_tuple_creation(text10_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb74b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text11 = wikipedia.summary('Aegon I Targaryen')\n",
    "file_path = os.path.join(folder_name, \"tt11.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text11)\n",
    "text11_edges_word_vec_ls = edge_tuple_creation(text11)\n",
    "text11_node_tup_ls = node_tuple_creation(text11_edges_word_vec_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea6c5dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text12 = wikipedia.summary('King Aegon IV Targaryen')\n",
    "file_path = os.path.join(folder_name, \"tt12.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text12)\n",
    "text12_edges_word_vec_ls = edge_tuple_creation(text12)\n",
    "text12_node_tup_ls = node_tuple_creation(text12_edges_word_vec_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6ec0497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text13 = wikipedia.summary('House Lannister ')\n",
    "file_path = os.path.join(folder_name, \"tt13.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text13)\n",
    "text13_edges_word_vec_ls = edge_tuple_creation(text13)\n",
    "text13_node_tup_ls = node_tuple_creation(text13_edges_word_vec_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4af8dfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text14 = wikipedia.summary('Seven Kingdoms (A Song of Ice and Fire)')\n",
    "file_path = os.path.join(folder_name, \"tt14.txt\")\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(text14)\n",
    "text14_edges_word_vec_ls = edge_tuple_creation(text14)\n",
    "text14_node_tup_ls = node_tuple_creation(text14_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a8f3232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559 349\n"
     ]
    }
   ],
   "source": [
    "full_node_ls = fireandblood_node_tup_ls + text2_node_tup_ls+text3_node_tup_ls+text4_node_tup_ls+text5_node_tup_ls+ text6_node_tup_ls+text7_node_tup_ls+text8_node_tup_ls+text9_node_tup_ls+ text10_node_tup_ls+text11_node_tup_ls+text12_node_tup_ls+text13_node_tup_ls+text14_node_tup_ls\n",
    "full_edge_ls = fireandblood_edges_word_vec_ls + text2_edges_word_vec_ls+ text3_edges_word_vec_ls+ text4_edges_word_vec_ls+ text5_edges_word_vec_ls+ text6_edges_word_vec_ls+ text7_edges_word_vec_ls+ text8_edges_word_vec_ls+ text9_edges_word_vec_ls+ text10_edges_word_vec_ls+ text11_edges_word_vec_ls+ text12_edges_word_vec_ls+ text13_edges_word_vec_ls+ text14_edges_word_vec_ls\n",
    "full_dedup_node_tup_ls = dedup(full_node_ls)\n",
    "print(len(full_node_ls), len(full_dedup_node_tup_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13f6c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://52.91.35.107:7687\", name=\"neo4j\", password=\"explanations-abettors-guys\")\n",
    "nodes_matcher = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b615fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph:  728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/79 [00:00<?, ?it/s]C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2264\\284134757.py:36: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
      "  tx.commit()\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:50<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#full\n",
    "add_nodes(full_dedup_node_tup_ls)\n",
    "add_edges(full_edge_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a69d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_edge_ls[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f92cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82bdc41e",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "def get_word_vec_similarity(node1, node2, node_ls):\n",
    "    \n",
    "    node1_vec = [tup[4] for tup in node_ls if tup[0] == node1]\n",
    "    node2_vec = [tup[4] for tup in node_ls if tup[0] == node2]\n",
    "    \n",
    "    return cosine_similarity((np.array(node1_vec)).reshape(-1,1),((np.array(node2_vec).reshape(-1,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa80da",
   "metadata": {},
   "source": [
    "cs = get_word_vec_similarity('fire and blood', 'Fire &blood', dedup_node_tup_ls)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab216de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
